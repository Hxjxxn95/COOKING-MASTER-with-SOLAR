import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from PIL import Image
from operator import itemgetter

from RAG import embed_file, format_docs
from Prompt import RAG_PROMPT, General_PROMPT
from UploadImage import get_image
from History import print_history, add_history


LANGSERVE_ENDPOINT = "http://localhost:8000/llm/"

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± 
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")


st.set_page_config(page_title="COOKING MASTER with llama3", page_icon="ğŸ’¬")
st.title("COOKING MASTER with llama3")


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ì‚¬ì§„ì„ ì˜¬ë¦¬ì‹œë©´ ìŒì‹ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤!"),
    ]

with st.sidebar:
    file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt", "docx"],
    )
    
if 'conversation_memory' not in st.session_state:
    st.session_state.conversation_memory = ConversationBufferMemory(human_prefix="user", ai_prefix="ai")

# ë©”ëª¨ë¦¬ ê°ì²´ ì°¸ì¡°
memory = st.session_state.conversation_memory
conversation = ConversationChain(memory=memory, llm = RemoteRunnable(LANGSERVE_ENDPOINT))

if file:
    retriever = embed_file(file)

with st.sidebar:
    uploaded_image = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"], key="image")

if uploaded_image is not None:
    get_image(uploaded_image, conversation)

print_history()


if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        
        chat_container = st.empty()
        
        if file is not None:
            prompt = RAG_PROMPT
            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    "context": itemgetter("input")|retriever|format_docs,
                    "history": itemgetter("history"),
                    "input" : itemgetter("input")
                }
                | prompt
                | conversation.llm
                | StrOutputParser()
            )
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            answer = rag_chain.stream({
                    "history": str(memory.load_memory_variables({})['history']), \
                    "input": str(user_input)})
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            conversation.memory.save_context(inputs={"user": user_input}, outputs={"ai": "".join(chunks)})
            add_history("ai", "".join(chunks))
             
        else:
            prompt = General_PROMPT
            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            chain = prompt | conversation.llm | StrOutputParser()
            answer = chain.stream({"history": memory.load_memory_variables({})['history'], "input": user_input})  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            conversation.memory.save_context(inputs={"user": user_input}, outputs={"ai": "".join(chunks)})
            add_history("ai", "".join(chunks))
